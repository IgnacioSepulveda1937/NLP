{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e67edc-462a-4f87-aff9-2191675fa28c",
   "metadata": {},
   "source": [
    "# NLP Bootcamp Graded Datathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fecd6e-5ab9-4108-a436-0c6326139ec4",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c62a28-17d6-4c42-b107-17d66e9c5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Import svm model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e0821d-07ab-4dbc-af98-d1a9274d6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data\n",
    "df=pd.read_csv('Train_Dataset.csv')\n",
    "df_test_final=pd.read_csv('Test_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945592ce-55dc-4974-9ba7-722ea1a33873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43fb6fa-08b7-4bc7-ab62-03618909dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test, y_train, y_test = train_test_split(df_sample['headline'], df_sample['is_sarcastic'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4812c3e-a6bf-445d-a4d9-89172845cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funcion que hara el pre-processing\n",
    "import nltk\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "### Usamos la dada\n",
    "\n",
    "# remove some stopwords to capture negation in n-grams if possible\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('but')\n",
    "\n",
    "# load up a simple porter stemmer - nothing fancy\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "def simple_text_preprocessor(document): \n",
    "    # lower case\n",
    "    document = str(document).lower()\n",
    "    \n",
    "    # expand contractions\n",
    "    document = contractions.fix(document)\n",
    "    \n",
    "    # remove unnecessary characters\n",
    "    document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
    "    document = re.sub(r'nbsp', r'', document)\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    \n",
    "    # simple porter stemming\n",
    "    document = ' '.join([ps.stem(word) for word in document.split()])\n",
    "    \n",
    "    # stopwords removal\n",
    "    document = ' '.join([word for word in document.split() if word not in stop_words])\n",
    "    \n",
    "    return document\n",
    "\n",
    "stp = np.vectorize(simple_text_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cfb02f2-a873-482e-ad4c-e960a8f385c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(df_train)\n",
    "df_test=pd.DataFrame(df_test)\n",
    "## Tokenizamos y medimos sentimiento\n",
    "import string\n",
    "df_train['char_count'] = df_train['headline'].apply(len)\n",
    "df_train['word_count'] = df_train['headline'].apply(lambda x: len(x.split()))\n",
    "df_train['word_density'] = df_train['char_count'] / (df_train['word_count']+1)\n",
    "df_train['punctuation_count'] = df_train['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "df_train['title_word_count'] = df_train['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df_train['upper_case_word_count'] = df_train['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "\n",
    "## Test\n",
    "\n",
    "df_test['char_count'] = df_test['headline'].apply(len)\n",
    "df_test['word_count'] = df_test['headline'].apply(lambda x: len(x.split()))\n",
    "df_test['word_density'] = df_test['char_count'] / (df_test['word_count']+1)\n",
    "df_test['punctuation_count'] = df_test['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "df_test['title_word_count'] = df_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df_test['upper_case_word_count'] = df_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b417633-3e4d-41da-814f-3ae2e4d1fd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>clean_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12782</th>\n",
       "      <td>north dakota not heard from in 48 hours</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>north dakota not heard hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42915</th>\n",
       "      <td>report: it going to take way more than an inco...</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>report go take way inconceiv act violenc count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33043</th>\n",
       "      <td>states' rights rancher ryan bundy to run for n...</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>state right rancher ryan bundi run nevada gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>watching thousands march in his honor unlocks ...</td>\n",
       "      <td>85</td>\n",
       "      <td>13</td>\n",
       "      <td>6.071429</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>watch thousand march hi honor unlock deeper da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38782</th>\n",
       "      <td>during the debate, these two did the unthinkab...</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "      <td>5.461538</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dure debat two unthink unit countri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  char_count  \\\n",
       "12782            north dakota not heard from in 48 hours          39   \n",
       "42915  report: it going to take way more than an inco...         106   \n",
       "33043  states' rights rancher ryan bundy to run for n...          60   \n",
       "1121   watching thousands march in his honor unlocks ...          85   \n",
       "38782  during the debate, these two did the unthinkab...          71   \n",
       "\n",
       "       word_count  word_density  punctuation_count  title_word_count  \\\n",
       "12782           8      4.333333                  0                 0   \n",
       "42915          19      5.300000                  1                 0   \n",
       "33043          10      5.454545                  1                 0   \n",
       "1121           13      6.071429                  2                 0   \n",
       "38782          12      5.461538                  1                 0   \n",
       "\n",
       "       upper_case_word_count  \\\n",
       "12782                      0   \n",
       "42915                      0   \n",
       "33043                      0   \n",
       "1121                       0   \n",
       "38782                      0   \n",
       "\n",
       "                                          clean_headline  \n",
       "12782                        north dakota not heard hour  \n",
       "42915  report go take way inconceiv act violenc count...  \n",
       "33043  state right rancher ryan bundi run nevada gove...  \n",
       "1121   watch thousand march hi honor unlock deeper da...  \n",
       "38782                dure debat two unthink unit countri  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Incorporamos\n",
    "df_train['clean_headline']=stp(df_train['headline'].values)\n",
    "df_train.head()\n",
    "## Incorporamos\n",
    "df_test['clean_headline']=stp(df_test['headline'].values)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0461e59a-4265-4e3f-b5e0-4d4279624c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### textblob\n",
    "import textblob\n",
    "### Agregamos polarizacion\n",
    "x_train_snt_obj = df_train['clean_headline'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "df_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]\n",
    "df_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]\n",
    "\n",
    "x_test_snt_obj = df_test['clean_headline'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "df_test['Polarity'] = [obj.polarity for obj in x_test_snt_obj.values]\n",
    "df_test['Subjectivity'] = [obj.subjectivity for obj in x_test_snt_obj.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32358432-9607-4d3c-a458-a75191856112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es aqui\n",
      "llego\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.303810</td>\n",
       "      <td>0.774554</td>\n",
       "      <td>-0.673557</td>\n",
       "      <td>-0.268684</td>\n",
       "      <td>-0.260771</td>\n",
       "      <td>0.066795</td>\n",
       "      <td>0.172779</td>\n",
       "      <td>-0.403031</td>\n",
       "      <td>0.408120</td>\n",
       "      <td>0.226385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231208</td>\n",
       "      <td>-0.388830</td>\n",
       "      <td>-0.059136</td>\n",
       "      <td>0.295165</td>\n",
       "      <td>0.101622</td>\n",
       "      <td>0.135062</td>\n",
       "      <td>0.355925</td>\n",
       "      <td>0.145863</td>\n",
       "      <td>0.089161</td>\n",
       "      <td>-0.193855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044506</td>\n",
       "      <td>0.293990</td>\n",
       "      <td>-0.488525</td>\n",
       "      <td>-0.092189</td>\n",
       "      <td>-0.205447</td>\n",
       "      <td>0.203497</td>\n",
       "      <td>0.295079</td>\n",
       "      <td>-0.203258</td>\n",
       "      <td>0.111602</td>\n",
       "      <td>0.042084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045776</td>\n",
       "      <td>-0.155665</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.177495</td>\n",
       "      <td>0.147358</td>\n",
       "      <td>-0.313848</td>\n",
       "      <td>-0.032647</td>\n",
       "      <td>-0.324034</td>\n",
       "      <td>-0.253415</td>\n",
       "      <td>0.042611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.655577</td>\n",
       "      <td>0.168032</td>\n",
       "      <td>-1.016943</td>\n",
       "      <td>-0.021515</td>\n",
       "      <td>-0.078975</td>\n",
       "      <td>-0.326722</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>-0.360011</td>\n",
       "      <td>0.084657</td>\n",
       "      <td>-0.277446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133649</td>\n",
       "      <td>0.288467</td>\n",
       "      <td>0.378292</td>\n",
       "      <td>0.163612</td>\n",
       "      <td>-0.320457</td>\n",
       "      <td>0.301191</td>\n",
       "      <td>-0.567193</td>\n",
       "      <td>0.421744</td>\n",
       "      <td>0.052206</td>\n",
       "      <td>-0.327750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.237299</td>\n",
       "      <td>1.083139</td>\n",
       "      <td>-0.817316</td>\n",
       "      <td>0.221413</td>\n",
       "      <td>-0.772712</td>\n",
       "      <td>-0.274603</td>\n",
       "      <td>-0.158052</td>\n",
       "      <td>-0.393026</td>\n",
       "      <td>-0.237888</td>\n",
       "      <td>-0.431125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120174</td>\n",
       "      <td>-0.102359</td>\n",
       "      <td>0.137160</td>\n",
       "      <td>-0.616942</td>\n",
       "      <td>-0.298536</td>\n",
       "      <td>-1.019961</td>\n",
       "      <td>-0.418451</td>\n",
       "      <td>-0.285799</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>-0.156521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015390</td>\n",
       "      <td>-0.026583</td>\n",
       "      <td>-0.642687</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>-0.234826</td>\n",
       "      <td>-0.231822</td>\n",
       "      <td>-0.292112</td>\n",
       "      <td>-0.082536</td>\n",
       "      <td>0.147594</td>\n",
       "      <td>0.041181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023186</td>\n",
       "      <td>0.054781</td>\n",
       "      <td>0.116169</td>\n",
       "      <td>0.133509</td>\n",
       "      <td>-0.127743</td>\n",
       "      <td>0.074932</td>\n",
       "      <td>-0.102889</td>\n",
       "      <td>-0.311851</td>\n",
       "      <td>-0.086333</td>\n",
       "      <td>-0.034309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33191</th>\n",
       "      <td>-0.528557</td>\n",
       "      <td>0.055446</td>\n",
       "      <td>-0.511212</td>\n",
       "      <td>-0.099616</td>\n",
       "      <td>-0.415503</td>\n",
       "      <td>-0.467024</td>\n",
       "      <td>0.317173</td>\n",
       "      <td>-0.806385</td>\n",
       "      <td>0.248347</td>\n",
       "      <td>-0.324204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257286</td>\n",
       "      <td>-0.155096</td>\n",
       "      <td>0.151183</td>\n",
       "      <td>0.623935</td>\n",
       "      <td>-0.903026</td>\n",
       "      <td>0.988929</td>\n",
       "      <td>-0.605757</td>\n",
       "      <td>0.098954</td>\n",
       "      <td>0.221164</td>\n",
       "      <td>-0.674599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33192</th>\n",
       "      <td>-0.465899</td>\n",
       "      <td>0.216865</td>\n",
       "      <td>-1.110556</td>\n",
       "      <td>0.454970</td>\n",
       "      <td>-0.016452</td>\n",
       "      <td>0.106102</td>\n",
       "      <td>0.675830</td>\n",
       "      <td>-0.342329</td>\n",
       "      <td>0.184576</td>\n",
       "      <td>-0.214454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065440</td>\n",
       "      <td>0.452247</td>\n",
       "      <td>0.540340</td>\n",
       "      <td>-0.089920</td>\n",
       "      <td>0.150541</td>\n",
       "      <td>0.441495</td>\n",
       "      <td>-0.046136</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>-0.043862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33193</th>\n",
       "      <td>-0.313827</td>\n",
       "      <td>0.272641</td>\n",
       "      <td>-0.463410</td>\n",
       "      <td>-0.125754</td>\n",
       "      <td>-0.501690</td>\n",
       "      <td>-0.360684</td>\n",
       "      <td>0.383413</td>\n",
       "      <td>-0.404789</td>\n",
       "      <td>-0.313446</td>\n",
       "      <td>-0.120449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174827</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.131856</td>\n",
       "      <td>0.422402</td>\n",
       "      <td>-0.740867</td>\n",
       "      <td>-0.422372</td>\n",
       "      <td>-0.188971</td>\n",
       "      <td>0.102957</td>\n",
       "      <td>0.146369</td>\n",
       "      <td>-0.117667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33194</th>\n",
       "      <td>-0.327128</td>\n",
       "      <td>0.200882</td>\n",
       "      <td>-0.033271</td>\n",
       "      <td>-0.310661</td>\n",
       "      <td>-0.472285</td>\n",
       "      <td>0.071983</td>\n",
       "      <td>0.132854</td>\n",
       "      <td>-0.363859</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>-0.286393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077401</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.465520</td>\n",
       "      <td>0.070317</td>\n",
       "      <td>0.089489</td>\n",
       "      <td>-0.036740</td>\n",
       "      <td>-0.211541</td>\n",
       "      <td>0.014745</td>\n",
       "      <td>-0.327120</td>\n",
       "      <td>-0.198710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33195</th>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.195134</td>\n",
       "      <td>-0.602193</td>\n",
       "      <td>0.048842</td>\n",
       "      <td>-0.088577</td>\n",
       "      <td>0.028638</td>\n",
       "      <td>-0.154861</td>\n",
       "      <td>-0.000932</td>\n",
       "      <td>-0.051083</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>0.155628</td>\n",
       "      <td>-0.061656</td>\n",
       "      <td>-0.005152</td>\n",
       "      <td>0.088518</td>\n",
       "      <td>-0.414207</td>\n",
       "      <td>0.104318</td>\n",
       "      <td>-0.182610</td>\n",
       "      <td>-0.058177</td>\n",
       "      <td>-0.401830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33196 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.303810  0.774554 -0.673557 -0.268684 -0.260771  0.066795  0.172779   \n",
       "1      0.044506  0.293990 -0.488525 -0.092189 -0.205447  0.203497  0.295079   \n",
       "2     -0.655577  0.168032 -1.016943 -0.021515 -0.078975 -0.326722  0.105642   \n",
       "3     -0.237299  1.083139 -0.817316  0.221413 -0.772712 -0.274603 -0.158052   \n",
       "4      0.015390 -0.026583 -0.642687 -0.007902 -0.234826 -0.231822 -0.292112   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33191 -0.528557  0.055446 -0.511212 -0.099616 -0.415503 -0.467024  0.317173   \n",
       "33192 -0.465899  0.216865 -1.110556  0.454970 -0.016452  0.106102  0.675830   \n",
       "33193 -0.313827  0.272641 -0.463410 -0.125754 -0.501690 -0.360684  0.383413   \n",
       "33194 -0.327128  0.200882 -0.033271 -0.310661 -0.472285  0.071983  0.132854   \n",
       "33195  0.051000  0.195134 -0.602193  0.048842 -0.088577  0.028638 -0.154861   \n",
       "\n",
       "             7         8         9   ...        30        31        32  \\\n",
       "0     -0.403031  0.408120  0.226385  ... -0.231208 -0.388830 -0.059136   \n",
       "1     -0.203258  0.111602  0.042084  ...  0.045776 -0.155665  0.240640   \n",
       "2     -0.360011  0.084657 -0.277446  ... -0.133649  0.288467  0.378292   \n",
       "3     -0.393026 -0.237888 -0.431125  ... -0.120174 -0.102359  0.137160   \n",
       "4     -0.082536  0.147594  0.041181  ... -0.023186  0.054781  0.116169   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33191 -0.806385  0.248347 -0.324204  ...  0.257286 -0.155096  0.151183   \n",
       "33192 -0.342329  0.184576 -0.214454  ... -0.065440  0.452247  0.540340   \n",
       "33193 -0.404789 -0.313446 -0.120449  ...  0.174827  0.002461  0.131856   \n",
       "33194 -0.363859  0.036887 -0.286393  ... -0.077401 -0.002415  0.465520   \n",
       "33195 -0.000932 -0.051083  0.072655  ...  0.004738  0.155628 -0.061656   \n",
       "\n",
       "             33        34        35        36        37        38        39  \n",
       "0      0.295165  0.101622  0.135062  0.355925  0.145863  0.089161 -0.193855  \n",
       "1      0.177495  0.147358 -0.313848 -0.032647 -0.324034 -0.253415  0.042611  \n",
       "2      0.163612 -0.320457  0.301191 -0.567193  0.421744  0.052206 -0.327750  \n",
       "3     -0.616942 -0.298536 -1.019961 -0.418451 -0.285799  0.009022 -0.156521  \n",
       "4      0.133509 -0.127743  0.074932 -0.102889 -0.311851 -0.086333 -0.034309  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "33191  0.623935 -0.903026  0.988929 -0.605757  0.098954  0.221164 -0.674599  \n",
       "33192 -0.089920  0.150541  0.441495 -0.046136  0.021386  0.946875 -0.043862  \n",
       "33193  0.422402 -0.740867 -0.422372 -0.188971  0.102957  0.146369 -0.117667  \n",
       "33194  0.070317  0.089489 -0.036740 -0.211541  0.014745 -0.327120 -0.198710  \n",
       "33195 -0.005152  0.088518 -0.414207  0.104318 -0.182610 -0.058177 -0.401830  \n",
       "\n",
       "[33196 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size =40  # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "tokenized_corpus_train = [nltk.word_tokenize(doc) for doc in df_train.clean_headline]\n",
    "tokenized_corpus_test = [nltk.word_tokenize(doc) for doc in df_test.clean_headline]\n",
    "print('es aqui')\n",
    "ft_model_train = FastText(tokenized_corpus_train, vector_size=feature_size, \n",
    "                     window=window_context, min_count = min_word_count,\n",
    "                     sg=sg, sample=sample, epochs=1000)\n",
    "print('llego')\n",
    "ft_model_test = FastText(tokenized_corpus_test, vector_size=feature_size, \n",
    "                     window=window_context, min_count = min_word_count,\n",
    "                     sg=sg, sample=sample, epochs=1000)\n",
    "ft_model_test\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key )\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "# get document level embeddings\n",
    "ft_doc_features_train = averaged_word_vectorizer(corpus=tokenized_corpus_train, model=ft_model_train,\n",
    "                                             num_features=feature_size)\n",
    "ft_doc_features_test = averaged_word_vectorizer(corpus=tokenized_corpus_test, model=ft_model_test,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(ft_doc_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80330d4-d5a9-4a1e-a2a2-69261930ebe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.loc[:,pd.DataFrame(ft_doc_features_train).columns] = ft_doc_features_train\n",
    "df_test.loc[:,pd.DataFrame(ft_doc_features_test).columns] = ft_doc_features_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f55265-f3e8-4131-aad5-823b62d87d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1, random_state=42, solver='liblinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c9ddf6-1e8d-47de-b905-83dfeb07f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(df_train.drop(['clean_headline','headline'], axis=1).fillna(0), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d095b3-d8b1-45a5-862d-c07edddb1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f167ae76-c904-4212-af5f-a5f8bcf07d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=15, random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=15, random_state=0)\n",
    "clf.fit(df_train.drop(['clean_headline','headline'], axis=1).fillna(0), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65114efe-c5dd-45e6-a0fd-74ae17eb0b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "lda.fit(df_train.drop(['clean_headline','headline'], axis=1).fillna(0), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f173270-7cef-471d-a99b-d936f412347a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a svm Classifier\n",
    "svm_clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "svm_clf.fit(df_train.drop(['clean_headline','headline'], axis=1).fillna(0), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2703a79-7471-4cd4-a3f8-b69d828064f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.73      0.69      5947\n",
      "           1       0.64      0.56      0.60      5119\n",
      "\n",
      "    accuracy                           0.65     11066\n",
      "   macro avg       0.65      0.64      0.64     11066\n",
      "weighted avg       0.65      0.65      0.65     11066\n",
      "\n",
      "Random Forest: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68      5947\n",
      "           1       0.63      0.57      0.60      5119\n",
      "\n",
      "    accuracy                           0.64     11066\n",
      "   macro avg       0.64      0.64      0.64     11066\n",
      "weighted avg       0.64      0.64      0.64     11066\n",
      "\n",
      "LDA: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69      5947\n",
      "           1       0.64      0.57      0.60      5119\n",
      "\n",
      "    accuracy                           0.65     11066\n",
      "   macro avg       0.65      0.64      0.65     11066\n",
      "weighted avg       0.65      0.65      0.65     11066\n",
      "\n",
      "SVM: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69      5947\n",
      "           1       0.64      0.56      0.59      5119\n",
      "\n",
      "    accuracy                           0.65     11066\n",
      "   macro avg       0.65      0.64      0.64     11066\n",
      "weighted avg       0.65      0.65      0.65     11066\n",
      "\n",
      "GB: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70      5947\n",
      "           1       0.65      0.55      0.59      5119\n",
      "\n",
      "    accuracy                           0.65     11066\n",
      "   macro avg       0.65      0.65      0.65     11066\n",
      "weighted avg       0.65      0.65      0.65     11066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr.fit(df_train.drop(['clean_headline','headline'], axis=1).fillna(0), y_train)\n",
    "predictions = lr.predict(df_test.drop(['clean_headline','headline'], axis=1))\n",
    "predictions2 = clf.predict(df_test.drop(['clean_headline','headline'], axis=1))\n",
    "predictions3 = lda.predict(df_test.drop(['clean_headline','headline'], axis=1))\n",
    "predictions4 = svm_clf.predict(df_test.drop(['clean_headline','headline'], axis=1))\n",
    "predictions5 = gb.predict(df_test.drop(['clean_headline','headline'], axis=1))\n",
    "print('Logistic: \\n',classification_report(y_test, predictions))\n",
    "print('Random Forest: \\n',classification_report(y_test, predictions2))\n",
    "print('LDA: \\n',classification_report(y_test, predictions3))\n",
    "print('SVM: \\n',classification_report(y_test, predictions4))\n",
    "print('GB: \\n',classification_report(y_test, predictions5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbebaf9-1566-4559-a6b7-24959dfda5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                784       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,073\n",
      "Trainable params: 1,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2656/2656 [==============================] - 11s 4ms/step - loss: 0.5615 - accuracy: 0.7074 - val_loss: 0.5222 - val_accuracy: 0.7520\n",
      "Epoch 2/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.5160 - accuracy: 0.7502 - val_loss: 0.5083 - val_accuracy: 0.7571\n",
      "Epoch 3/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.5076 - accuracy: 0.7531 - val_loss: 0.5150 - val_accuracy: 0.7538\n",
      "Epoch 4/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.5025 - accuracy: 0.7561 - val_loss: 0.4996 - val_accuracy: 0.7556\n",
      "Epoch 5/1000\n",
      "2656/2656 [==============================] - 8s 3ms/step - loss: 0.5004 - accuracy: 0.7587 - val_loss: 0.4978 - val_accuracy: 0.7566\n",
      "Epoch 6/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4981 - accuracy: 0.7573 - val_loss: 0.4967 - val_accuracy: 0.7586\n",
      "Epoch 7/1000\n",
      "2656/2656 [==============================] - 12s 4ms/step - loss: 0.4927 - accuracy: 0.7644 - val_loss: 0.4919 - val_accuracy: 0.7617\n",
      "Epoch 8/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4894 - accuracy: 0.7656 - val_loss: 0.4882 - val_accuracy: 0.7648\n",
      "Epoch 9/1000\n",
      "2656/2656 [==============================] - 8s 3ms/step - loss: 0.4837 - accuracy: 0.7693 - val_loss: 0.4887 - val_accuracy: 0.7654\n",
      "Epoch 10/1000\n",
      "2656/2656 [==============================] - 8s 3ms/step - loss: 0.4801 - accuracy: 0.7689 - val_loss: 0.5099 - val_accuracy: 0.7536\n",
      "Epoch 11/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4765 - accuracy: 0.7721 - val_loss: 0.4863 - val_accuracy: 0.7661\n",
      "Epoch 12/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4742 - accuracy: 0.7715 - val_loss: 0.4815 - val_accuracy: 0.7670\n",
      "Epoch 13/1000\n",
      "2656/2656 [==============================] - 11s 4ms/step - loss: 0.4708 - accuracy: 0.7758 - val_loss: 0.4894 - val_accuracy: 0.7630\n",
      "Epoch 14/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.4694 - accuracy: 0.7764 - val_loss: 0.4844 - val_accuracy: 0.7628\n",
      "Epoch 15/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4679 - accuracy: 0.7763 - val_loss: 0.4759 - val_accuracy: 0.7753\n",
      "Epoch 16/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4639 - accuracy: 0.7796 - val_loss: 0.4851 - val_accuracy: 0.7669\n",
      "Epoch 17/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4627 - accuracy: 0.7804 - val_loss: 0.4782 - val_accuracy: 0.7735\n",
      "Epoch 18/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.4610 - accuracy: 0.7799 - val_loss: 0.4723 - val_accuracy: 0.7744\n",
      "Epoch 19/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.4601 - accuracy: 0.7820 - val_loss: 0.4926 - val_accuracy: 0.7613\n",
      "Epoch 20/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4579 - accuracy: 0.7822 - val_loss: 0.4708 - val_accuracy: 0.7797\n",
      "Epoch 21/1000\n",
      "2656/2656 [==============================] - 8s 3ms/step - loss: 0.4567 - accuracy: 0.7848 - val_loss: 0.4739 - val_accuracy: 0.7756\n",
      "Epoch 22/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4554 - accuracy: 0.7871 - val_loss: 0.4740 - val_accuracy: 0.7747\n",
      "Epoch 23/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.4540 - accuracy: 0.7840 - val_loss: 0.4748 - val_accuracy: 0.7755\n",
      "Epoch 24/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.4539 - accuracy: 0.7871 - val_loss: 0.4700 - val_accuracy: 0.7762\n",
      "Epoch 25/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4532 - accuracy: 0.7861 - val_loss: 0.4717 - val_accuracy: 0.7765\n",
      "Epoch 26/1000\n",
      "2656/2656 [==============================] - 9s 3ms/step - loss: 0.4523 - accuracy: 0.7851 - val_loss: 0.4726 - val_accuracy: 0.7753\n",
      "Epoch 27/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4510 - accuracy: 0.7864 - val_loss: 0.4793 - val_accuracy: 0.7735\n",
      "Epoch 28/1000\n",
      "2656/2656 [==============================] - 10s 4ms/step - loss: 0.4506 - accuracy: 0.7869 - val_loss: 0.4749 - val_accuracy: 0.7747\n",
      "Epoch 29/1000\n",
      "2656/2656 [==============================] - 11s 4ms/step - loss: 0.4494 - accuracy: 0.7884 - val_loss: 0.4725 - val_accuracy: 0.7750\n",
      "Epoch 30/1000\n",
      "2656/2656 [==============================] - 11s 4ms/step - loss: 0.4495 - accuracy: 0.7888 - val_loss: 0.4683 - val_accuracy: 0.7782\n",
      "[[14941  3070]\n",
      " [ 3986 11199]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81     18011\n",
      "           1       0.78      0.74      0.76     15185\n",
      "\n",
      "    accuracy                           0.79     33196\n",
      "   macro avg       0.79      0.78      0.78     33196\n",
      "weighted avg       0.79      0.79      0.79     33196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Neural Network\n",
    "X=df_train.drop(['clean_headline','headline'], axis=1).fillna(0).to_numpy()\n",
    "Y= y_train.to_numpy()\n",
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# for modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# build a model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(X.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping callback\n",
    "# This callback will stop the training when there is no improvement in  \n",
    "# the validation loss for 10 consecutive epochs.  \n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                                   mode='max', # don't minimize the accuracy!\n",
    "                                   patience=10,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "# now we just update our model fit call\n",
    "history = model.fit(X,\n",
    "                    Y,\n",
    "                    callbacks=[es],\n",
    "                    epochs=1000, # you can set this to a big number!\n",
    "                    batch_size=10,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "X_test=df_test.drop(['clean_headline','headline'],axis=1).to_numpy()\n",
    "# see how these are numbers between 0 and 1? \n",
    "model.predict(X_test) # prob of successes (survival)\n",
    "\n",
    "\n",
    "\n",
    "# so we need to round to a whole number (0 or 1),\n",
    "# or the confusion matrix won't work!\n",
    "preds = np.round(model.predict(X),0)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(Y, preds)) # order matters! (actual, predicted)\n",
    "\n",
    "\n",
    "print(classification_report(Y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e1a75-fad0-4087-80a4-65cebe45330d",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fb41571-2135-416b-bd9a-b0ce20acbe3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (33196) does not match length of index (44262)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IGNACI~1.SEP\\AppData\\Local\\Temp/ipykernel_12448/2778179216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft_doc_features_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mft_doc_features_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft_doc_features_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mft_doc_features_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1728\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m             \u001b[1;31m# We have to operate column-wise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1730\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1731\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1768\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1769\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_2d_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1771\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlplane_indexer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_2d_value\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1833\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1834\u001b[0m             \u001b[1;31m# setting with a list, re-coerces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1835\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1837\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_frame_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   1922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \u001b[1;31m# reset the sliced object if unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iset_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_single_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_iset_item\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   3763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iset_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3765\u001b[1;33m         \u001b[0marraylike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3766\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iset_item_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marraylike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (33196) does not match length of index (44262)"
     ]
    }
   ],
   "source": [
    "### Paso 1\n",
    "\n",
    "### Funcion que hara el pre-processing\n",
    "import nltk\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "### Usamos la dada\n",
    "\n",
    "# remove some stopwords to capture negation in n-grams if possible\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('but')\n",
    "\n",
    "# load up a simple porter stemmer - nothing fancy\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "def simple_text_preprocessor(document): \n",
    "    # lower case\n",
    "    document = str(document).lower()\n",
    "    \n",
    "    # expand contractions\n",
    "    document = contractions.fix(document)\n",
    "    \n",
    "    # remove unnecessary characters\n",
    "    document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
    "    document = re.sub(r'nbsp', r'', document)\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    \n",
    "    # simple porter stemming\n",
    "    document = ' '.join([ps.stem(word) for word in document.split()])\n",
    "    \n",
    "    # stopwords removal\n",
    "    document = ' '.join([word for word in document.split() if word not in stop_words])\n",
    "    \n",
    "    return document\n",
    "\n",
    "stp = np.vectorize(simple_text_preprocessor)\n",
    "\n",
    "## Paso2\n",
    "\n",
    "df_train=pd.DataFrame(df) ### Estrenamos todo el trainning set \n",
    "df_test=pd.DataFrame(df_test_final)\n",
    "## Tokenizamos y medimos sentimiento\n",
    "import string\n",
    "df_train['char_count'] = df_train['headline'].apply(len)\n",
    "df_train['word_count'] = df_train['headline'].apply(lambda x: len(x.split()))\n",
    "df_train['word_density'] = df_train['char_count'] / (df_train['word_count']+1)\n",
    "df_train['punctuation_count'] = df_train['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "df_train['title_word_count'] = df_train['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df_train['upper_case_word_count'] = df_train['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "\n",
    "## Test\n",
    "\n",
    "df_test['char_count'] = df_test['headline'].apply(len)\n",
    "df_test['word_count'] = df_test['headline'].apply(lambda x: len(x.split()))\n",
    "df_test['word_density'] = df_test['char_count'] / (df_test['word_count']+1)\n",
    "df_test['punctuation_count'] = df_test['headline'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "df_test['title_word_count'] = df_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df_test['upper_case_word_count'] = df_test['headline'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "## Incorporamos\n",
    "df_train['clean_headline']=stp(df_train['headline'].values)\n",
    "df_train.head()\n",
    "## Incorporamos\n",
    "df_test['clean_headline']=stp(df_test['headline'].values)\n",
    "df_test.head()\n",
    "\n",
    "## Incorporamos\n",
    "df_train['clean_headline']=stp(df_train['headline'].values)\n",
    "df_train.head()\n",
    "## Incorporamos\n",
    "df_test['clean_headline']=stp(df_test['headline'].values)\n",
    "df_test.head()\n",
    "\n",
    "df_train.loc[:,pd.DataFrame(ft_doc_features_train).columns] = ft_doc_features_train\n",
    "df_test.loc[:,pd.DataFrame(ft_doc_features_test).columns] = ft_doc_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec208dd-264e-4d04-bc79-19711206695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## resultado\n",
    "\n",
    "## Neural Network\n",
    "X=df_train.drop(['clean_headline','headline'], axis=1).fillna(0).to_numpy()\n",
    "Y= y_train.to_numpy()\n",
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# for modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# build a model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(X.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping callback\n",
    "# This callback will stop the training when there is no improvement in  \n",
    "# the validation loss for 10 consecutive epochs.  \n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                                   mode='max', # don't minimize the accuracy!\n",
    "                                   patience=10,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "# now we just update our model fit call\n",
    "history = model.fit(X,\n",
    "                    Y,\n",
    "                    callbacks=[es],\n",
    "                    epochs=1000, # you can set this to a big number!\n",
    "                    batch_size=10,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "X_test=df_test.drop(['clean_headline','headline'],axis=1).to_numpy()\n",
    "# see how these are numbers between 0 and 1? \n",
    "model.predict(X_test) # prob of successes (survival)\n",
    "\n",
    "\n",
    "\n",
    "# so we need to round to a whole number (0 or 1),\n",
    "# or the confusion matrix won't work!\n",
    "preds = np.round(model.predict(X),0)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(Y, preds)) # order matters! (actual, predicted)\n",
    "\n",
    "\n",
    "print(classification_report(Y, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64748fd3-0840-4fab-aac5-25bac525c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final['predicted']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdad5a-fe7b-4f17-b6f5-9ea331e29405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
